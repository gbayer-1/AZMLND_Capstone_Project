{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning using HyperDrive\n",
        "\n",
        "TODO: Import Dependencies. In the cell below, import all the dependencies that you will need to complete the project."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment\n",
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.exceptions import ComputeTargetException\n",
        "from azureml.data.dataset_factory import TabularDatasetFactory\n",
        "from azureml.widgets import RunDetails"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1635172455795
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "TODO: Get data. In the cell below, write code to access the data you will be using in this project. Remember that the dataset needs to be external."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup workspace and experiment"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\n",
        "\n",
        "# print some information about the workspace\n",
        "print('Workspace name: ' + ws.name, \n",
        "      'Azure region: ' + ws.location, \n",
        "      'Subscription id: ' + ws.subscription_id, \n",
        "      'Resource group: ' + ws.resource_group, sep = '\\n')\n",
        "\n",
        "# choose a name for experiment\n",
        "experiment_name = 'heart-failure-experiment'\n",
        "\n",
        "experiment=Experiment(ws, experiment_name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Workspace name: quick-starts-ws-161946\nAzure region: southcentralus\nSubscription id: aa7cf8e8-d23f-4bce-a7b9-1f0b4e0ac8ee\nResource group: aml-quickstarts-161946\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1635172456888
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a cluster\n",
        "\n",
        "Reuse the AutoML cluster or create a new one, if it doesn't exist"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_name = \"expcluster\"\n",
        "\n",
        "# Use existing cluster, if it exists\n",
        "try:\n",
        "    compute_target = ComputeTarget(workspace=ws, name = cluster_name)\n",
        "    print('Found existing cluster, use it!')\n",
        "except ComputeTargetException:\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_DS12_v2',\n",
        "                                                          max_nodes=6, min_nodes=1)\n",
        "    compute_target = ComputeTarget.create(workspace=ws, name=cluster_name, provisioning_configuration=compute_config)\n",
        "compute_target.wait_for_completion(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found existing cluster, use it!\nSucceeded\nAmlCompute wait for completion finished\n\nMinimum number of nodes requested have been provisioned\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1635172457525
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Dataset\n",
        "Reuse the dataset from the AutoML run or upload it, if it doesn't exist."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from train import read_data\n",
        "dataset=read_data()\n",
        "df = dataset.to_pandas_dataframe()\n",
        "df.describe()"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'_OfflineRun' object has no attribute 'experiment'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0170307516c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pandas_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/notebook161946/code/Users/odl_user_161946/AZMLND_Capstone_Project-master/train.py\u001b[0m in \u001b[0;36mread_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     '''\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mws\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mdata_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"heart_disease_data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mdescription_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"heart_failure_prediction_dataset from kaggle\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_OfflineRun' object has no attribute 'experiment'"
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1635160670524
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperdrive Configuration\n",
        "\n",
        "TODO: Explain the model you are using and the reason for chosing the different hyperparameters, termination policy and config settings."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598531923519
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.train.hyperdrive.policy import BanditPolicy\n",
        "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
        "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
        "from azureml.train.hyperdrive.parameter_expressions import uniform, choice, randint\n",
        "\n",
        "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
        "\n",
        "#TODO: Create the different params that you will be using during training\n",
        "param_sampling = RandomParameterSampling({\"--n_estimators\": randint(10000),\n",
        "    \"--max_depth\": choice(10, 100, 1000, 5000),\n",
        "    \"--min_samples_split\": choice(2,20,25,50)})\n",
        "\n",
        "from azureml.core import Environment\n",
        "from azureml.core import ScriptRunConfig\n",
        "\n",
        "env = Environment.from_conda_specification(name = 'env', file_path = './envs/conda_environment.yml')\n",
        "\n",
        "src = ScriptRunConfig(source_directory = \"./\",\n",
        "    script = \"train.py\",\n",
        "    compute_target = \"expcluster\",\n",
        "    environment = env)\n",
        "\n",
        "hyperdrive_run_config = HyperDriveConfig(run_config=src,\n",
        "    hyperparameter_sampling=param_sampling,\n",
        "    policy=early_termination_policy,\n",
        "    primary_metric_name=\"Accuracy\",\n",
        "    primary_metric_goal= PrimaryMetricGoal.MAXIMIZE,\n",
        "    max_total_runs=20,\n",
        "    max_concurrent_runs=5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1635171721505
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Submit your experiment\n",
        "hyperdrive_run = experiment.submit(hyperdrive_run_config)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1635171726711
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Details\n",
        "\n",
        "OPTIONAL: Write about the different models trained and their performance. Why do you think some models did better than others?\n",
        "\n",
        "TODO: In the cell below, use the `RunDetails` widget to show the different experiments."
      ],
      "metadata": {
        "collapsed": true,
        "gather": {
          "logged": 1598544898497
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RunDetails(hyperdrive_run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1635171727797
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperdrive_run.get_children_sorted_by_primary_metric()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635172310442
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Best Model\n",
        "\n",
        "TODO: In the cell below, get the best model from the hyperdrive experiments and display all the properties of the model."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperdrive_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "print(\"best run details: \", best_hyperdrive_run.get_details())\n",
        "print(\"best run metrics :\", best_hyperdrive_run.get_metrics())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1635172319595
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Save the best model\n",
        "for f in best_hyperdrive_run.get_file_names():\n",
        "    if f.startswith('outputs/hyperdrive_model.onnx'):\n",
        "        output_file_path = os.path.join('./model', 'hyperdrive_model.onnx')\n",
        "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
        "        best_hyperdrive_run.download_file(name=f, output_file_path=output_file_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downloading from outputs/hyperdrive_model.onnx to ./model/hyperdrive_model.onnx ...\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "gather": {
          "logged": 1635172404915
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperdrive_run.get_file_names()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 56,
          "data": {
            "text/plain": "['azureml-logs/55_azureml-execution-tvmps_d962ebf8dd84ca36aa3417869ac656d6530c05b96c2d00749484d1ec5e169bf8_d.txt',\n 'azureml-logs/65_job_prep-tvmps_d962ebf8dd84ca36aa3417869ac656d6530c05b96c2d00749484d1ec5e169bf8_d.txt',\n 'azureml-logs/70_driver_log.txt',\n 'azureml-logs/75_job_post-tvmps_d962ebf8dd84ca36aa3417869ac656d6530c05b96c2d00749484d1ec5e169bf8_d.txt',\n 'azureml-logs/process_info.json',\n 'azureml-logs/process_status.json',\n 'logs/azureml/97_azureml.log',\n 'logs/azureml/dataprep/backgroundProcess.log',\n 'logs/azureml/dataprep/backgroundProcess_Telemetry.log',\n 'logs/azureml/job_prep_azureml.log',\n 'logs/azureml/job_release_azureml.log',\n 'outputs/hyperdrive_model.onnx']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635172334881
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert to ONNX Format"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "Remember you have to deploy only one of the two models you trained but you still need to register both the models. Perform the steps in the rest of this notebook only if you wish to deploy this model.\n",
        "\n",
        "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get environment yml and scoring scripts\r\n",
        "for f in best_hyperdrive_run.get_file_names():\r\n",
        "    if f.__contains__(\"scoring\"):\r\n",
        "        output_file_path = os.path.join('./model', os.path.basename(f))\r\n",
        "        print('Downloading from {} to {} ...'.format(f, output_file_path))\r\n",
        "        best_hyperdrive_run.download_file(name=f, output_file_path=output_file_path)\r\n",
        "    if f.__contains__(\"conda_env\"):\r\n",
        "        output_file_path = os.path.join('./model', os.path.basename(f))\r\n",
        "        print('Downloading from {} to {} ...'.format(f, output_file_path))\r\n",
        "        best_automl_hyperdrive.download_file(name=f, output_file_path=output_file_path)"
      ],
      "outputs": [],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635167234474
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_hyperdrive_run.properties"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 32,
          "data": {
            "text/plain": "{'_azureml.ComputeTargetType': 'amlcompute',\n 'ContentSnapshotId': 'ef2c10b8-6218-4527-a550-71a0c769c601',\n 'ProcessInfoFile': 'azureml-logs/process_info.json',\n 'ProcessStatusFile': 'azureml-logs/process_status.json'}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1635167553584
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell is how to register the .pkl model\n",
        "description = \"HyperDrive run heart-failure classification model\"\n",
        "\n",
        "from azureml.core import Model\n",
        "model = Model.register(workspace=ws,\n",
        "    model_name=\"hyperdrive_model\",\n",
        "    model_path='./model/hyperdrive_model.onnx',\n",
        "    model_framework=Model.Framework.ONNX,\n",
        "    model_framework_version='1.3',\n",
        "    description=description)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Registering model hyperdrive_model\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1635172529854
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import InferenceConfig\n",
        "from azureml.core.webservice import AciWebservice\n",
        "from azureml.core import Model\n",
        "\n",
        "service_name = 'heart-failure-hyperdrive-service'\n",
        "\n",
        "inference_config = InferenceConfig(entry_script='model/scoring_file_v_1_0_0.py')\n",
        "aci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1,\n",
        "                                                enable_app_insights=True,\n",
        "                                                auth_enabled=True)"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1635172612877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy to local for debugging\n",
        "from azureml.core.webservice import LocalWebservice\n",
        "deployment_config = LocalWebservice.deploy_configuration(port=6789)\n",
        "test_service = Model.deploy(\n",
        "    ws,\n",
        "    name='test-service',\n",
        "    models=[model],\n",
        "    inference_config=inference_config,\n",
        "    deployment_config=deployment_config,\n",
        "    overwrite=True\n",
        ")\n",
        "test_service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'lower'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0420c96e5fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdeployment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[0mtest_service\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_deployment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(workspace, name, models, inference_config, deployment_config, deployment_target, overwrite, show_output)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             return deployment_config._webservice_type._deploy(workspace, name, models,\n\u001b[1;32m   1645\u001b[0m                                                               \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                                                               deployment_config=deployment_config)\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m         \u001b[0;31m# IotWebservice does not support environment-style deployment,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(workspace, name, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    741\u001b[0m                        \u001b[0minference_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minference_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m                        \u001b[0mdeployment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m                        wait=wait)\n\u001b[0m\u001b[1;32m    744\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mservice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 raise WebserviceException('Cannot call {}() when service is {}.'.format(func.__name__, self.state),\n\u001b[1;32m     71\u001b[0m                                           logger=module_logger)\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/webservice/local.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, models, image_config, deployment_config, wait, inference_config)\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0;31m# Old runtime and ImageConfig-based path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_image_conf_for_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/model.py\u001b[0m in \u001b[0;36m_convert_to_image_conf_for_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2157\u001b[0m                                     \u001b[0mjoinPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_docker_file_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2158\u001b[0m                                     \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2159\u001b[0;31m                                     self.base_image, self.base_image_registry, True, self.cuda_version)\n\u001b[0m\u001b[1;32m   2160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/image/container.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, execution_script, runtime, conda_file, docker_file, schema_file, dependencies, enable_gpu, tags, properties, description, base_image, base_image_registry, allow_absolute_path, cuda_version)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_script_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_script\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_configuration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_local_debug_payload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/core/image/container.py\u001b[0m in \u001b[0;36mvalidate_configuration\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mvalidate_entry_script_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecution_script_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSUPPORTED_RUNTIMES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0mruntimes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'|'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSUPPORTED_RUNTIMES\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mUNDOCUMENTED_RUNTIMES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             raise WebserviceException('Provided runtime not supported. '\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'lower'"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "service = Model.deploy(workspace=ws,\n",
        "                       name=service_name,\n",
        "                       models=[model],\n",
        "                       inference_config=inference_config,\n",
        "                       deployment_config=aci_config,\n",
        "                       overwrite=True)\n",
        "service.wait_for_deployment(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, send a request to the web service you deployed to test it."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# get some testdata to send a request\n",
        "data = df.head(1).drop(\"HeartDisease\", axis=1).to_json(orient='records')\n",
        "body = {\"data\": data,\n",
        "       \"method\": \"predict\"}\n",
        "print(body)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import json, requests\n",
        "# test against local deploymenz\n",
        "uri = test_service.scoring_uri\n",
        "requests.get(\"http://localhost:6789\")\n",
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "response = requests.post(uri, data=json.dumps(body), headers=headers)\n",
        "print(response.json())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# local deployment is working\n",
        "test_service.delete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# send request to deployed web service\n",
        "uri = service.scoring_uri\n",
        "print(uri)\n",
        "print(service.swagger_uri)\n",
        "key, _ = service.get_keys()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "headers = {\"Content-Type\": \"application/json\"}\n",
        "headers[\"Authorization\"] = f\"Bearer {key}\"\n",
        "response = requests.post(uri, data=json.dumps(body), headers=headers)\n",
        "print(response.json())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "TODO: In the cell below, print the logs of the web service and delete the service"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(service.get_logs())\n",
        "service.delete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Checklist**\n",
        "- I have registered the model.\n",
        "- I have deployed the model with the best accuracy as a webservice.\n",
        "- I have tested the webservice by sending a request to the model endpoint.\n",
        "- I have deleted the webservice and shutdown all the computes that I have used.\n",
        "- I have taken a screenshot showing the model endpoint as active.\n",
        "- The project includes a file containing the environment details.\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}